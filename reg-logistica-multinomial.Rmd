---
title: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, eval = FALSE)
```

# 1. Introdução

# 2. Metodologia

O modelo de regressão logística multinomial pertence à classe dos modelos lineares generalizados. Nesse modelo, a variável resposta $Y$ é categórica com mais de duas categorias. Suponha que a variável resposta tenha $K$ categorias e temos um vetor $x$ de covariáveis. Construímos o modelo de regressão multinomial através dos $K-1$ logitos

$$
\log \frac{\operatorname{Pr}(Y=\ell \mid x)}{\operatorname{Pr}(Y=K \mid x)}=\beta_{0 \ell}+x^{\top} \beta_{\ell}, \ell=1, \ldots, K-1,
$$
onde $\beta_\ell$ é vetor de coeficientes de dimensão $p$. De maneira equivalente, modelamos

$$
\operatorname{Pr}(Y=\ell \mid x)=\frac{e^{\beta_{0 \ell}+x^{\top} \beta_{\ell}}}{\sum_{k=1}^{K} e^{\beta_{0 k}+x^{\top} \beta_{k}}}.
$$

Essa parametrização não é estimável sem restrições, porque para qualquer valor dos parâmetros $\left\{\beta_{0 \ell}, \beta_{\ell}\right\}_{1}^{K},\left\{\beta_{0 \ell}-c_{0}, \beta_{\ell}-c\right\}_{1}^{K}$ fornecem as mesmas probabilidades (referencia). A Regularização lida com essa ambiguidade de maneira natural.

Agora precisamos construir a função onde estimamos os parâmetros do modelo. Primeiro construímos a função de verossimilhança para distribuição multinomial. O desenvolvimento dessa função é baseado em (referência). Então, considere que temos $N$ subpopulações em nossos dados, onde cada subpopulação é representada por um vetor de covariáveis $x_i$, $i = 1,\ldots,N$, tamanho $n_i$ e $y_\ell$ observações na categoria $\ell = 1,\ldots,K$. Considere ainda que $\pi_{i\ell} = \operatorname{Pr}(Y=\ell \mid x_i)$. Logo, a função densidade de probabilidade conjunta é:

$$
f(\boldsymbol{y} \mid \boldsymbol{\beta})=\prod_{i=1}^{N}\left[\frac{n_{i} !}{\prod_{j=1}^{J} y_{i j} !} \cdot \prod_{j=1}^{J} \pi_{i j}^{y_{i j}}\right],
$$
onde $\boldsymbol{\beta}$ é o vetor de parâmetros do modelo de regressão. Como queremos maximizar a equação acima com respeito à $\boldsymbol{\beta}$, os termos fatoriais que não têm os termos $\pi_{ij}$ podem ser tratados como constantes. Então, podemos escrever a função de verossimilhança para o modelo de regressão logística multinomial como:

$$
L(\boldsymbol{\beta} \mid \boldsymbol{y}) \simeq \prod_{i=1}^{N} \prod_{j=1}^{J} \pi_{i j}^{y_{i j}}.
$$
Note que $\sum_{j=1}^{J} y_{i j} = n_{i} \Leftrightarrow y_{i J} = n_{i}-\sum_{j=1}^{J-1} y_{i j}$. A equação acima torna-se:

$$
\begin{aligned}
&\prod_{i=1}^{N} \prod_{j=1}^{J-1} \pi_{i j}^{y_{i j}} \cdot \pi_{i J}^{n_{i}-\sum_{j=1}^{J-1} y_{i j}} \\
=& \prod_{i=1}^{N} \prod_{j=1}^{J-1} \pi_{i j}^{y_{i j}} \cdot \frac{\pi_{i J}^{n_{i}}}{\pi_{i J} \sum_{j=1}^{J-1} y_{i j}} \\
=& \prod_{i=1}^{N} \prod_{j=1}^{J-1} \pi_{i j}^{y_{i j}} \cdot \frac{\pi_{i J}^{n_{i}}}{\prod_{j=1}^{J-1} \pi_{i J} y_{i j}}\\
=& \prod_{i=1}^{N} \prod_{j=1}^{J-1}\left(\frac{\pi_{i j}}{\pi_{i J}}\right)^{y_{i j}} \cdot \pi_{i J}^{n_{i}}.
\end{aligned}
$$
Da Equação (), pode ser mostrado que 

$$
\pi_{i J}=\frac{1}{1+\sum_{j=1}^{J-1} e^{\sum_{k=0}^{K} x_{i k} \beta_{k j}}}.
$$

Agora, substituindo $\pi_{ij}$ e $\pi_{iJ}$ usando as Equações () e (), temos:

$$
\begin{aligned}
L(\boldsymbol{\beta} \mid \boldsymbol{y}) \simeq & \prod_{i=1}^{N} \prod_{j=1}^{J-1}\left(e^{\sum_{k=0}^{K} x_{i k} \beta_{k j}}\right)^{y_{i j}} \cdot\left(\frac{1}{1+\sum_{j=1}^{J-1} e^{K_{k=0}^{K} x_{i k} \beta_{k j}}}\right)^{n_{i}} \\
=& \prod_{i=1}^{N} \prod_{j=1}^{J-1} e^{y_{i j} \sum_{k=0}^{K} x_{i k} \beta_{k j}} \cdot\left(1+\sum_{j=1}^{J-1} e^{\sum_{k=0}^{K} x_{i k} \beta_{k j}}\right)^{-n_{i}}.
\end{aligned}
$$

Com isso, a função de logverossimilhança para o modelo de regressão logística multinomial é:

$$
l(\boldsymbol{\beta})=\sum_{i=1}^{N} \sum_{j=1}^{J-1}\left(y_{i j} \sum_{k=0}^{K} x_{i k} \beta_{k j}\right)-n_{i} \log \left(1+\sum_{j=1}^{J-1} e^{\sum_{k=0}^{K} x_{i k} \beta_{k j}}\right).
$$

Para o modelo de regressão logística multinomial regularizada, estimamos os parâmetros do modelo através da função de logverossimilhança penalizada, que é dada por:

$$
l(\boldsymbol{\beta},\boldsymbol{\lambda},\boldsymbol{\alpha}) = l(\boldsymbol{\beta}) - \boldsymbol{\lambda}P_{\boldsymbol{\alpha}}(\boldsymbol{\beta}),
$$
onde

$$
\begin{aligned}
P_{\alpha}(\beta) &=(1-\alpha)\|\beta\|_{\ell_{2}}^{2}+\alpha\|\beta\|_{\ell_{1}} \\
&=\sum_{j=1}^{p}\left[(1-\alpha) \beta_{j}^{2}+\alpha\left|\beta_{j}\right|\right].
\end{aligned}
$$
$P_{\alpha}(\beta)$ é a penalização *elastic-net*. Nesse trabalho consideramos a penalização ridge ($\alpha = 0$) e a penalização lasso ($\alpha = 1$).

Portanto, para cada valor de $\lambda$, encontramos os estimadores de $\beta$ ao maximizar a logverossimilhança penalizada. Vale ressaltar que os valores de $\beta$ são encontrados através de métodos numéricos, pois não há solução analítica para encontrar os estimadores de $\beta$.

Lembrar de comentar sobre as variáveis padronizadas

## 2.1 Selecionando o melhor valor de $\lambda$

Os valores dos estimadores de $\beta$ são obtidos para um acordo com o valor de $\lambda$. Logo, para escolher um $\lambda$ ótimo, podemos usar um erro de predição para nos guiar. Nesse trabalho, separamos os dados em treinamento e teste, sendo 70% dos dados para treino e 30% para teste. Nos dados de treino, aplicamos a regressão ridge e lasso, e escolhemos o valor de $\lambda$ através da validação cruzada. O conjunto de teste é usado para avaliar os modelos de regressão ridge e lasso nos dados de treinamento. Além disso, como as classes do nosso banco de dados são balanceadas, usamos o erro de classificação como o erro de predição.

# 3. Simulação

De acordo com (referência), as regressões ridge e lasso são úteis quando temos mais variáveis preditoras do que observações, ou qualquer situação onde há muitas variáveis preditoras correlacionadas. (referência) ainda argumenta que a regressão ridge é conhecida por encolher os coeficientes de preditores correlacionados, um em direção à outro, como se tivessem o mesmo peso no modelo. Por sua vez, a regressão lasso é, de certa maneira, indiferente à preditores altamente correlacionados, e tende a escolher um preditor e ignorar o restante.

Com isso, vamos construir a regressão ridge e lasso para dois casos: com preditores correlacionados e preditores maior que observação. A variável resposta é multinomial com 3 classes, cada classe rotulada como 1, 2 e 3, respectivamente. 

## 3.1 Preditoras correlacionadas

Por simplicidade, consideramos apenas dez preditoras, geradas a partir de uma distribuição normal, com 1000 repetições, e introduzimos correlação entre algumas dessas variáveis. Para o i-ésimo vetor de preditoras $x_i$, seja $\pi_{ij} = P(Y = j \mid x_i)$, $j = 1,2,3$, onde Y é a variável resposta. Então, consideramos o modelo:

$$
\begin{aligned}
\log \left(\frac{\pi_{i1}}{\pi_{i3}}\right) &= 1 + x_i^T\beta_1 = 1 + 2x_{i1} + 3x_{i2} - x_{i3} + 4x_{i4} + 10x_{i5} - 2x_{i6} - 4x_{i7} + 7x_{i8} - 2x_{i9} + x_{i10},\\
\log \left(\frac{\pi_{i2}}{\pi_{i3}}\right) &= 3 + x_i^T\beta_2 = 3 + x_{i1} - x_{i2} - 5x_{i3} + 2x_{i4} + 15x_{i5} + 3x_{i6} - 7x_{i7} + 4x_{i8} + 6x_{i9} + 9x_{i10},
\end{aligned}
$$

onde

$$
\begin{aligned}
\pi_{i 1} &=\frac{e^{1 + x_i^T\beta_1}}{1+e^{(1 + x_i^T\beta_1) + (3 + x_i^T\beta_2)}} \\
\pi_{i 2} &=\frac{e^{3 + x_i^T\beta_2}}{1+e^{(1 + x_i^T\beta_1) + (3 + x_i^T\beta_2)}} \\
\pi_{i 3} &=\frac{1}{1+e^{(1 + x_i^T\beta_1) + (3 + x_i^T\beta_2)}}.
\end{aligned}
$$

A i-ésima observação da variável resposta foi gerada a partir de uma distribuição multinomial com as proporções obtidas na Equação (acima). Esse processo resultou em 340 variáveis com valor 1, 452 variáveis com valor 2 e 208 variáveis com valor 3. A Figura 1 mostra a estrutura de correlação entre as 10 variáveis, onde algumas estão altamente correlacionadas entre si.

```{r}
library(HTLR)
library(clusterGeneration)
```

```{r eval = TRUE}
set.seed(1)
x1 = rnorm(1000)
x2 = x1 + rnorm(1000, sd = 0.4)
x3 = -(x1 + rnorm(1000, sd = 1))
x4 = rnorm(1000)
x5 = x1 + rnorm(1000, sd = 0.5)
x6 = rnorm(1000)
x7 = x3 + rnorm(1000, sd = 0.4)
x8 = rnorm(1000)
x9 = x8 + rnorm(1000, sd = 10)
x10 = x8 + rnorm(1000, sd = 0.5)
x.B1 = 1 + 2*x1 + 3*x2 - x3 + 4*x4 + 10*x5 - 2*x6 - 4*x7 + 7*x8 - 2*x9 + x10
x.B2 = 3 + x1 - x2 - 5*x3 + 2*x4 + 15*x5 + 3*x6 - 7*x7 + 4*x8 + 6*x9 + 9*x10
p1 = exp(x.B1)/(1 + exp(x.B1) + exp(x.B2))
p2 = exp(x.B2)/(1 + exp(x.B1) + exp(x.B2))
p3 = 1/(1 + exp(x.B1) + exp(x.B2))
#p1 = exp(x.B1)/(1 + exp(x.B1))
#p2 = 1 - p1
p = data.frame(p1, p2, p3)
y = apply(p, 1, function(x) rmultinom(n = 1:2, size = 1, prob = x)) |> 
  apply(2, function(x) which.max(x))
#table(y)
dados = data.frame(y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
cor.matrix = cor(dados[-1])
colnames(cor.matrix) = c("$x[1]", "$x[2]", "$x[3]", "$x[4]", "$x[5]",
                         "$x[6]", "$x[7]", "$x[8]", "$x[9]", "$x[10]")
rownames(cor.matrix) = c("$x[1]", "$x[2]", "$x[3]", "$x[4]", "$x[5]",
                         "$x[6]", "$x[7]", "$x[8]", "$x[9]", "$x[10]")
corrplot::corrplot(cor.matrix, tl.pos = 'd', cl.pos = 'n', 
                   addCoef.col = 'black', method = 'color')
```

O modelo obtido por regressão ridge fornece o seguinte resultado:

```{r}
set.seed(1)
dados1 = dados
preProcValues = preProcess(dados1 |> select(-y), method = c("scale", "center"))
dados1_pad = predict(preProcValues, dados1)
x = model.matrix(y ~ ., dados1_pad)
y = dados1_pad |> 
  select(y) |> 
  unlist() |> 
  as.numeric()
cv.out = cv.glmnet(x, y, alpha = 0,
                   family = "multinomial", 
                   type.measure = "class") # Fit ridge regression model on training data
bestlam_ridge = cv.out$lambda.min  # Select lambda that minimizes training misclassification error
ridge_mod = glmnet(x, y, alpha = 0, family = "multinomial")
d = predict(ridge_mod, type = "coefficients", s = 10)
```

```{r}
((d$`1` |> as.vector())[c(1,3:12)] - (d$`3` |> as.vector())[c(1,3:12)]) |> round(3)
((d$`2` |> as.vector())[c(1,3:12)] - (d$`3` |> as.vector())[c(1,3:12)]) |> round(3)
```

$$
\begin{aligned}
\log \left(\frac{\pi_{i1}}{\pi_{i3}}\right) & = 0,826 + 0,431z_{i1} + 0,422z_{i2} - 0,315z_{i3} + 0,341z_{i4} + 0,863z_{i5} - 0,227z_{i6} - 0,417z_{i7} + 0,528z_{i8} - 1,176z_{i9} + 0,260z_{i10},\\
\log \left(\frac{\pi_{i2}}{\pi_{i3}}\right) & = 1,301 + 0,392z_{i1} + 0,256z_{i2} - 0,431z_{i3} + 0,121z_{i4} + 0,822z_{i5} + 0,06z_{i6} - 0,482z_{i7} + 0,434z_{i8} + 1,845z_{i9} + 0,422z_{i10}.
\end{aligned}
$$

onde $z_{ij}$ é a variável padronizada. O $\lambda$ escolhido pela validação cruzada é 0,0404. No geral, não vemos muitos coeficientes encolhidos para próximo de zero e o peso das variáveis correlacionadas não estão próximos. Isso pode ser explicado pelo valor de $\lambda$ ser próximo de zero. Se aumentarmos o valor de $lambda$ para, digamos, dez, temos o seguinte modelo:

$$
\begin{aligned}
\log \left(\frac{\pi_{i1}}{\pi_{i3}}\right) & = 0,495 + 0,028z_{i1} + 0,028z_{i2} - 0,025z_{i3} + 0,008z_{i4} + 0,030z_{i5} - 0,006z_{i6} - 0,025z_{i7} + 0,009z_{i8} - 0,025z_{i9} + 0,009z_{i10},\\
\log \left(\frac{\pi_{i2}}{\pi_{i3}}\right) & = 0,780 + 0,028z_{i1} + 0,025z_{i2} - 0,026z_{i3} + 0,002z_{i4} + 0,028z_{i5} - 0,001z_{i6} - 0,025z_{i7} + 0,018z_{i8} + 0,04z_{i9} + 0,016z_{i10}.
\end{aligned}
$$

Agora os resultados aproximam-se daqueles comentado por (ref): os coeficientes foram encolhidos para próximo de zero e os coeficientes das variáveis altamente correlacionadas estão próximos uns dos outros.

Por sua vez, o modelo obtido por regressão lasso fornece o seguinte resultado:

```{r}
#set.seed(1)
#cv.out = cv.glmnet(x, y, alpha = 1,
#                   family = "multinomial", 
#                   type.measure = "class") # Fit lasso regression model on training data
#bestlam_lasso = cv.out$lambda.min  # Select lambda that minimizes training misclassification error
lasso_mod = glmnet(x, y, alpha = 1, family = "multinomial")
d = predict(lasso_mod, type = "coefficients", s = 0.1) # 0.1

((d$`1` |> as.vector())[c(1,3:12)] - (d$`3` |> as.vector())[c(1,3:12)]) |> round(3)
((d$`2` |> as.vector())[c(1,3:12)] - (d$`3` |> as.vector())[c(1,3:12)]) |> round(3)
```

$$
\begin{aligned}
\log \left(\frac{\pi_{i1}}{\pi_{i3}}\right) & = 0,401 + 0,033z_{i1} + 0z_{i2} - 0,106z_{i3} + 0z_{i4} + 0,634z_{i5} - 0z_{i6} - 0,08z_{i7} + 0z_{i8} - 0,543z_{i9} + 0z_{i10},\\
\log \left(\frac{\pi_{i2}}{\pi_{i3}}\right) & = 0,712 + 0,033z_{i1} + 0z_{i2} - 0,106z_{i3} + 0z_{i4} + 0,634z_{i5} - 0z_{i6} - 0,08z_{i7} + 0z_{i8} + 1,333z_{i9} + 0z_{i10}.
\end{aligned}
$$

Não usamos o $\lambda$ obtido pela validação cruzada, pois era praticamente zero e as estimativas do modelo eram próximas ao modelo sem penalização. Consideramos, então, $\lambda = 0.1$. Não consideramos o mesmo valor de $\lambda$ da regressão ridge ($\lambda$ = 10), pois mesmo para valores pequenos de $\lambda$, os coeficientes das preditoras eram zeros. Dito isso, os resultados do modelo com $\lambda = 0.1$ aproximam-se daqueles comentado por (ref): entre as preditoras altamente correlacionadas, o modelo manteve uma e discartou o restante.

## 3.1 Quantidade de preditoras maior que observações

Nesse caso não avaliamos a forma das estimativas do coeficientes do modelo de regressão como no caso das preditoras correlacionadas. O principal argumento para considerar a regressão penalizada por ridge ou lasso quando a quantidade de preditoras é maior que as observações, é que a estimativa do coeficientes é única, enquanto que a regressão sem penalização não tem solução única para a estimativa dos coeficientes. Portanto, não simulamos dados nesse cenário, pois o interesse não está em saber os valores dos coeficientes.

# 4. Aplicação em dados reais

```{r eval = FALSE}
x = read.csv("imageMNIST.csv")
y = read.csv("labelMNIST.csv")
colnames(x) = paste0("pixel.", 1:400)
colnames(y) = "label"
y$label = factor(y$label)
dados = cbind(y, x)
```

```{r eval = FALSE}
library(caret)
library(tidymodels)
library(glmnet)
```

```{r eval = FALSE}
set.seed(1)

split_teste <- initial_split(dados, prop = 0.7, strata = label)

#dados teste
teste <- split_teste |> testing()

# dados treinamento
treino <- split_teste |> training()
```

```{r eval = FALSE}
treino$label |> table()
```

```{r eval = FALSE}
set.seed(1)
x_treino = model.matrix(label ~ ., treino)
y_treino = treino %>%
  select(label) %>%
  unlist() %>%
  as.numeric()
cv.out = cv.glmnet(x_treino, y_treino, alpha = 0, 
                   family = "multinomial", type.measure = "class") # Fit ridge regression model on training data
bestlam_ridge = cv.out$lambda.min  # Select lambda that minimizes training misclassification error
bestlam_ridge
```

```{r eval = FALSE}
x_teste = model.matrix(label ~ ., teste)
y_teste = teste %>%
  select(label) %>%
  unlist() %>%
  as.numeric()
ridge_mod = glmnet(x_treino, y_treino, alpha = 0, family = "multinomial")
ridge_pred = predict(ridge_mod, s = bestlam_ridge, newx = x_teste) # Use best lambda to predict test data
class.prediction = apply(ridge_pred[,,1], 1, function(x) which.max(x))
misc.error = sum(class.prediction == y_teste) / length(y_teste)
misc.error
```

```{r eval = FALSE}
set.seed(1)
cv.out = cv.glmnet(x_treino, y_treino, alpha = 1, 
                   family = "multinomial", type.measure = "class") # Fit lasso regression model on training data
bestlam_lasso = cv.out$lambda.min  # Select lambda that minimizes training misclassification error
bestlam_lasso
```

```{r eval = FALSE}
lasso_mod = glmnet(x_treino, y_treino, alpha = 1, family = "multinomial")
lasso_pred = predict(lasso_mod, s = bestlam_lasso, newx = x_teste) # Use best lambda to predict test data
class.prediction = apply(lasso_pred[,,1], 1, function(x) which.max(x))
misc.error = sum(class.prediction == y_teste) / length(y_teste)
misc.error
```











